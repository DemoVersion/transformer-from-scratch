# Custom PyTorch stub file to declare missing attributes
from typing import Any, Union, Optional, Tuple, List, TypeVar, Iterator, overload
import numpy as np

_T = TypeVar('_T', bound='Tensor')
_device = Union[str, Any]

class Size(Tuple[int, ...]):
    @overload
    def __getitem__(self, key: slice) -> "Size": ...  # type: ignore[override]
    @overload
    def __getitem__(self, key: int) -> int: ...  # type: ignore[override]

# Tensor types
class Tensor:
    def cuda(self: _T) -> _T: ...
    def item(self) -> Union[int, float]: ...
    @overload
    def size(self, dim: int) -> int: ...
    @overload  
    def size(self, dim: None = None) -> Size: ...
    def argmax(self, dim: Optional[int] = None) -> Tensor: ...
    def sum(self, dim: Optional[int] = None) -> Tensor: ...
    def detach(self: _T) -> _T: ...
    def clone(self: _T) -> _T: ...
    def expand(self: _T, *sizes: int) -> _T: ...
    def view(self: _T, *shape: int) -> _T: ...
    def transpose(self: _T, dim0: int, dim1: int) -> _T: ...
    def contiguous(self: _T) -> _T: ...
    def to(self: _T, device: _device) -> _T: ...
    def __getitem__(self, key: Any) -> Tensor: ...
    def __iter__(self) -> Iterator[Tensor]: ...
    def __mul__(self: _T, other: Union[float, int, Tensor]) -> _T: ...
    def __add__(self: _T, other: Union[float, int, Tensor]) -> _T: ...
    def __sub__(self: _T, other: Union[float, int, Tensor]) -> _T: ...
    def __truediv__(self: _T, other: Union[float, int, Tensor]) -> _T: ...
    def __neg__(self: _T) -> _T: ...
    def __rmul__(self: _T, other: Union[float, int]) -> _T: ...
    def __radd__(self: _T, other: Union[float, int]) -> _T: ...
    def __rsub__(self: _T, other: Union[float, int]) -> _T: ...
    def __rtruediv__(self: _T, other: Union[float, int]) -> _T: ...
    def uniform_(self: _T, from_: float, to: float) -> _T: ...

# Missing torch functions  
def no_grad() -> Any: ...
def manual_seed(seed: int) -> None: ...
def from_numpy(ndarray: Any) -> Tensor: ...
def frombuffer(buffer: Union[bytes, Any], dtype: Any, count: int = -1, offset: int = 0) -> Tensor: ...
def cat(tensors: List[Any], dim: int = 0) -> Tensor: ...
def randint(low: int, high: int, size: Tuple[int, ...], **kwargs: Any) -> Tensor: ...
def randn(*size: int, **kwargs: Any) -> Tensor: ...
def arange(start: Union[int, float], end: Optional[Union[int, float]] = None, step: Union[int, float] = 1, **kwargs: Any) -> Tensor: ...
def zeros(*size: int, **kwargs: Any) -> Tensor: ...
def ones(*size: int, **kwargs: Any) -> Tensor: ...
def empty(*size: int, **kwargs: Any) -> Tensor: ...
def tensor(data: Any, **kwargs: Any) -> Tensor: ...
def bmm(input: Any, mat2: Any) -> Tensor: ...
def einsum(equation: str, *operands: Any) -> Tensor: ...
def addmm(bias: Any, input: Any, mat2: Any) -> Tensor: ...
def matmul(input: Any, other: Any) -> Tensor: ...
def triu_indices(row: int, col: int, offset: int = 0, **kwargs: Any) -> Tensor: ...
def log_softmax(input: Any, dim: int, **kwargs: Any) -> Tensor: ...

# Missing torch attributes
long: Any

# CUDA availability check
class _CudaModule:
    def is_available(self) -> bool: ...

cuda: _CudaModule

# Utilities module (for compatibility)
class _UtilsModule:
    class data:
        class DataLoader: ...
        class Dataset: ...
    class tensorboard:
        class SummaryWriter: ...

utils: _UtilsModule