# PyTorch nn module stub file
from typing import Any, Optional, Union, Callable

class Module:
    def __init__(self) -> None: ...
    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...
    def forward(self, *args: Any, **kwargs: Any) -> Any: ...
    def parameters(self) -> Any: ...
    def train(self, mode: bool = True) -> Module: ...
    def cuda(self) -> Module: ...
    def to(self, *args: Any, **kwargs: Any) -> Module: ...

class Linear(Module):
    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None: ...

class Embedding(Module):
    def __init__(self, num_embeddings: int, embedding_dim: int, **kwargs: Any) -> None: ...

class LayerNorm(Module):
    def __init__(self, normalized_shape: Any, **kwargs: Any) -> None: ...

class Sequential(Module):
    def __init__(self, *args: Module) -> None: ...

class ReLU(Module):
    def __init__(self, inplace: bool = False) -> None: ...

class Dropout(Module):
    def __init__(self, p: float = 0.5, inplace: bool = False) -> None: ...

class Softmax(Module):
    def __init__(self, dim: Optional[int] = None) -> None: ...

class Parameter:
    def __init__(self, data: Any) -> None: ...
    def expand(self, *sizes: int) -> Parameter: ...
    def cuda(self) -> Parameter: ...
    def to(self, device: Any) -> Parameter: ...
    def contiguous(self) -> Parameter: ...
    def view(self, *shape: int) -> Parameter: ...

class utils:
    @staticmethod
    def clip_grad_norm_(parameters: Any, max_norm: float) -> None: ...

# nn.init module
class init:
    @staticmethod
    def normal_(tensor: Any, mean: float = 0, std: float = 1) -> Any: ...
    @staticmethod
    def constant_(tensor: Any, val: float) -> Any: ...
    @staticmethod
    def xavier_uniform_(tensor: Any, gain: float = 1) -> Any: ...
    @staticmethod
    def kaiming_uniform_(tensor: Any, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu') -> Any: ...
    @staticmethod
    def _calculate_fan_in_and_fan_out(tensor: Any) -> Any: ...
    @staticmethod
    def uniform_(tensor: Any, a: float, b: float) -> Any: ...

# Import commonly used functions
from torch.nn import functional as F